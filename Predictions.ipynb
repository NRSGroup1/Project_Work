{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# regular expressione and tools for sentiment analysis\n",
    "from textblob import TextBlob\n",
    "import re \n",
    "\n",
    "import pickle # to save the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummification(df):\n",
    "    #_______________ Categorical Dummification\n",
    "    categorical = df.select_dtypes(exclude=np.number).columns.tolist()\n",
    "    df = pd.get_dummies(df, columns = categorical, prefix = categorical)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to clean the tweets\n",
    "def cleanTxt(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('@[A-Za-z0–9]+', '', text) #Removing @mentions\n",
    "    text = re.sub('#', '', text) # Removing '#' hash tag\n",
    "    text = re.sub('RT[\\s]+', '', text) # Removing RT\n",
    "    text = re.sub('https?:\\/\\/\\S+', '', text) # Removing hyperlink\n",
    "    text = re.sub('&amp;', '', text) #remove ampersand\n",
    "    text = re.sub('\\n',' ', text) #remove breakline\n",
    "    text = re.sub(\"[^\\w]\", \" \",  text) #remove all distinct to word\n",
    "    text = re.sub('å', '', text)\n",
    "    text = re.sub('ä', '', text)\n",
    "    text = re.sub('ā', '', text)\n",
    "\n",
    "\n",
    "    text = re.sub(' +', ' ', text) #remove multiple spaces\n",
    "\n",
    "    # clean emoticons\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanData(df_crm, df_finance, df_sales, df_twitter):\n",
    "    \n",
    "    # column id renaming to join the datasets\n",
    "    df_crm = df_crm.rename(columns={'ID_CRM': 'id'}) \n",
    "    df_finance = df_finance.rename(columns={'ID_FINANCE': 'id'})\n",
    "    df_sales = df_sales.rename(columns={'ID_SALES': 'id'})\n",
    "    df_twitter = df_twitter.rename(columns={'ID_SALES': 'id'})\n",
    "    \n",
    "    \n",
    "    # df_crm unique_id extraction\n",
    "    df_crm.id = np.where(df_crm.Income_Level.str.len() == 2,\\\n",
    "                     df_crm.id.str.slice(start = 1, stop = -2),\\\n",
    "                     df_crm.id.str.slice(start = 1, stop = -1))\n",
    "\n",
    "    # df_finance unique_id extraction\n",
    "    df_finance.id = np.where(df_finance.Special_Pay.str.len() == 2,\\\n",
    "                         df_finance.id.str.slice(start = 2),\\\n",
    "                         df_finance.id.str.slice(start = 0))\n",
    "\n",
    "    # df_sales unique_id extraction\n",
    "    df_sales.id = np.where(df_sales.Program_Code.str.len() == 2,\\\n",
    "                       df_sales.id.str.slice(start = 2, stop = -1),\\\n",
    "                       df_sales.id.str.slice(start = 3, stop = -1))\n",
    "\n",
    "    # df_twitter unique_id extraction\n",
    "    df_twitter.id = df_twitter.id.str.extract(r'(\\d+)', expand=False)\n",
    "    \n",
    "    \n",
    "    df_twitter['text'] = df_twitter['text'].apply(cleanTxt)\n",
    "    #get the sentiment of each tweet first, then calculate the mean\n",
    "\n",
    "    df_twitter_merged = df_twitter.copy()\n",
    "    df_twitter_merged['polarity'] = df_twitter_merged['text'].apply(lambda x: TextBlob(x).sentiment[0])\n",
    "    df_twitter_merged['subjectivity'] = df_twitter_merged['text'].apply(lambda x: TextBlob(x).sentiment[1])\n",
    "\n",
    "    df_twitter_merged = df_twitter_merged.groupby('id').mean()\n",
    "    \n",
    "    # create a single table to join the datasets\n",
    "    df_merged = df_sales.merge(df_crm, on=\"id\", how=\"inner\").merge(df_finance, on=\"id\", how=\"inner\").merge(df_twitter_merged, on=\"id\", how=\"left\")\n",
    "    \n",
    "    \n",
    "      #______________ id\n",
    "    df_merged.drop(columns=['id'], inplace=True)\n",
    "\n",
    "    \n",
    "    #______________ From_Grade\n",
    "    #missing values replaced with the median\n",
    "    df_merged['From_Grade'] = df_merged['From_Grade'].fillna(df_merged['From_Grade'].median())\n",
    "\n",
    "\n",
    "    #______________ To_Grade\n",
    "    #missing values replaced with the median (with respect to the subgroup)\n",
    "    df_merged['To_Grade'] = df_merged.groupby(['From_Grade'], sort=False)['To_Grade'].apply(lambda x: x.fillna(x.median()))\n",
    "    df_merged['To_Grade'] = df_merged['To_Grade'].fillna(df_merged['To_Grade'].median())\n",
    "\n",
    "\n",
    "    #____**NEW**____ Delta_From_To_Grade\n",
    "    #Distance from lowest to highest grade in school of participants. When zero it represents an indicator for the trip\n",
    "    #taken by a group comprising students from the same grade.\n",
    "    df_merged['Delta_From_To_Grade'] = df_merged['To_Grade'] - df_merged['From_Grade']\n",
    "\n",
    "    \n",
    "    #_______________ Polarity\n",
    "    df_merged['polarity'].fillna(df_merged['polarity'].mean(), inplace=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #____**NEW**____ Total_Pax_rev\n",
    "    df_merged['Total_Pax_rev'] = np.log(df_merged['FPP'] + df_merged['Total_Discount_Pax'])\n",
    "    \n",
    "        \n",
    "    df_merged = dummification(df_merged) \n",
    "    \n",
    "    #Retained is the target \n",
    "    Selected_Features = ['polarity',\n",
    "                     'Delta_From_To_Grade',\n",
    "                     'Total_Pax_rev',\n",
    "                     'SPR_Product_Type_East Coast',\n",
    "                    ]\n",
    "    \n",
    "    return df_merged[Selected_Features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "## Load the model\n",
    "filename = 'DecisionTreeClassifierModel.sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "## Load datasets\n",
    "\n",
    "path = '20210222_generali_project_test/'\n",
    "missing_values = [''] #define values to be identified as NaN in the datasets\n",
    "df_crm = pd.read_csv(path + 'crm_test.csv', keep_default_na=False, na_values=missing_values, decimal=',') #import CRM_test\n",
    "df_finance = pd.read_csv(path + 'finance_test.csv', keep_default_na=False, na_values=missing_values, decimal=',') #import finance_test\n",
    "df_sales = pd.read_csv(path + 'sales_test.csv', keep_default_na=False, na_values=missing_values, decimal=',') #import sales_test\n",
    "df_twitter = pd.read_csv(path + 'twitter_test.csv', keep_default_na=False, na_values=missing_values, decimal=',') #import twitter_test\n",
    "\n",
    "#clean data\n",
    "X_test = cleanData(df_crm, df_finance, df_sales, df_twitter)\n",
    "\n",
    "#execute the model\n",
    "predictions = loaded_model.predict(X_test)\n",
    "\n",
    "#save the result\n",
    "pd.DataFrame(predictions).to_csv(path + \"predictions.csv\", index = False, header = False)\n",
    "\n",
    "print('OK')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
